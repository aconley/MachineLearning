\font\big=cmbx10 scaled\magstep1
\font\tfont=cmr10 scaled\magstep3

\topglue 0.5in
\centerline{\tfont Notes on Elements of Statistical Learning}
\vskip 0.5in

\centerline{\big Chapter 4}
\vskip 0.2in

\noindent
{\bf Section 4.2}
In the discussion around equation (4.4), relating to linear classification,
it is claimed that it must  be the case that  $\sum_{k \in {\cal G}} {\hat f}_k\left(x\right) = 1$
as long as there is a column of ones in the {\bf X}.  This seems like
nonsense, but a little experimentation shows it is true -- why?

An important point is the the sum of each row in $\hat {\bf B}$ is zero
except for the first one, which sums to one.  The fact that the sum of
the $\hat f_k$ is unity follows from this immediately.  But why do 
the sums work the way they do?

Consider the formula for $\hat {\bf B}$:
$
\hat {\bf B} = \left({\bf X}^T {\bf X}\right)^{-1} {\bf X}^T {\bf Y}.
$
Next consider that each row of ${\bf Y}$ sums to one.
Then, the row sum of ${\bf B}$ satisfies
$\sum_j \hat{\bf B}_{ij} = \left({\bf X}^T {\bf X}\right)^{-1} {\bf X}^T {\bf 1}$.
This is just the equation for doing a multi-linear fit to a constant vector
of all 1s, where the obvious solution will have an intercept of 1 and
zero for all other coefficients.

\vskip 0.2in
\noindent
{\bf Section 4.5.1}
The peceptron algorithm minimizes a quantity
$$
D\left(\beta, \beta_0\right) = - \sum_{i \in {\cal M}} y_i \left(x_i^T \beta + \beta_0\right)
$$
which is only proportional to the distance to the hyperplane.  However,
the separating proportionality factor is $1 / ||\beta||$ (see eq.\ 4.40),
so as written this doesn't quite work.  One possibility is that the algorithm
requires $||\beta|| = 1$, since that defines the same separating plan; but
the update steps don't require this, so there seems to be a problem here.

\vskip 0.4in
\centerline{\big Chapter 5}
\vskip 0.2in
\noindent
{\bf Section 5.2} The mathermatical notation $_+$ isn't really explained, but
is common.  It is defined as
$$
 x_+ = \cases{ x, & if $x > 0$;\cr 0, & otherwise \cr }.
$$
Thus, something like $\left(x - \xi\right)_+$ is a unit slope line starting
at $x = \xi$ and 0 below that.

\vskip 0.2in
{\bf Section 5.4.1}
After eq.\ 5.15, it is stated that ${\bf H}_{\xi} {\bf H}_{\xi} = {\bf H}_{\xi}$.
The definition is ${\bf H}_{\xi} = {\bf B}_{\xi} \left( {\bf B}_{\xi}^T
 {\bf B}_{\xi}\right)^{-1} {\bf B}_{\xi}^T$.  So 
 ${\bf H}_{\xi} {\bf H}_{\xi} = {\bf B}_{\xi} \left({\bf B}_{\xi}^T {\bf B}_{\xi}
 \right)^{-1} {\bf B}_{\xi}^T {\bf B}_{\xi} \left({\bf B}_{\xi}^T {\bf B}_{\xi} 
 \right)^{-1} {\bf B}_{\xi}^T =
 {\bf B}_{\xi} \left({\bf B}_{\xi} {\bf B}_{\xi} \right)^{-1} \left({\bf B}_{\xi}^T
 {\bf B}_{\xi}\right) \left({\bf B}_{\xi} {\bf B}_{\xi} \right)^{-1} {\bf B}_{\xi}^T =
 {\bf H}_{\xi},$
 as claimed.  Note that this is only non-trivial because ${\bf B}_{\xi}$
 is not square -- if it were, then ${\bf H}_{\xi}$ would be the identity anyways.
 
 \vskip 0.2in
 {\bf Section 5.6}
Equation 5.30: $y \log p + \left(1 - y\right) \log \left(1-p\right) =
y \log e^f - y \log \left(1 + e^f\right) - \left(1 - y\right) \log \left(1 + e^f\right) =
y f - \log \left(1 + e^f\right)$, as claimed.

What about equation 5.31?  We have
$
{\partial \ell \over \partial \theta} = y {\partial f \over \partial \theta} - p 
{\partial f \over \partial \theta}$ minus a term for the derivative (see 5.11).
Then substitute ${\partial f \over \partial \theta} = {\bf N}$.
5.32 is basically the same thing.

\vskip 0.4in
\centerline{\big Chapter 7}
\vskip 0.2in
\noindent
{\bf Section 7.3} The bias variance decomposition takes some work to show,
but it's important.  The assumption is that $Y = f\left(x\right) + \epsilon$ with
$E\left(\epsilon\right) = 0$, and our estimate of $f$ is $\hat f$.  Dropping
the arguments from $f$ and ${\hat f}$ (so $f\left(x\right)$ is just $f$), at a point $x_0$,
$$
 \eqalign{
   E\left[ \left(Y - {\hat f}\right)^2 \right] 
    =& \, E\left[ \left(Y - f + f - {\hat f} \right)^2 \right] \cr
    =& \, E\left[ \left(Y - f\right)^2 \right] + 
            E\left[\left(f\ - {\hat f}\right)^2\right] +
         2 E\left[\left(f - {\hat f}\right)\,\left(Y - f\right)\right] \cr
    =& \, E\left[ \left(Y -f\right)^2 \right] + 
            E\left[\left(f - {\hat f}\right)^2\right] +
         2 \left( E\left[f {\hat f}\right] + E\left[Y f\right] 
                   -E \left[Y {\hat f}\right] - E\left[f^2\right]\right).
}
$$
Now, $Y - f = \epsilon$, so $E\left[\left(Y - f\right)^2\right] = E\left[\epsilon^2\right]$,
and since $E\left[\epsilon\right] = 0$, this is just ${\rm Var}\left[\epsilon\right]$.
Next, $E\left[y {\hat f}\right] = E\left[ f {\hat f}\right] + E\left[ \epsilon {\hat f}\right]$.
${\hat f}$ is deterministic and independent of $\epsilon$, so $E\left[\epsilon {\hat f}\right] = 0$.
Thus the first and third parts of the final term cancel.  Similarly, $E\left[ y f \right] = f^2$
and $E\left[ f^2\right] = f^2$ so the second and fourth also cancel.  Therefore,
$$
 E\left[ \left(Y - {\hat f}\right)^2 \right] = {\rm Var}\left[\epsilon\right] + 
 E\left[\left(f - {\hat f}\right)^2\right].
 $$
 
 Next, apply the same type of expansion to the final term but inserting $E\left[{\hat f}\right]$:
 $$
 \eqalign{
   E\left[ \left(f - {\hat f}\right)^2 \right] 
      =& \, E\left[ \left(f - E\left[{\hat f}\right] + E\left[{\hat f}\right] - {\hat f}\right)^2\right] \cr
      =& \, E\left[ \left(f - E\left[{\hat f}\right]\right)^2\right] + 
               E\left[\left(E\left[{\hat f}\right] - {\hat f}\right)^2\right]
              + 2 E\left[ \left(E\left[{\hat f}\right] - {\hat f}\right)\,\left(f - E\left[{\hat f}\right]\right)\right] .
 } 
 $$

As in the previous expansion, the final term turns out to cancel.  
$E\left[f E\left[{\hat f}\right]\right] = f E\left[{\hat f}\right]$ since $f$ is deterministic.
$E\left[ f {\hat f}\right] = f E\left[{\hat f}\right]$, so these terms cancel.
$E\left[E\left[{\hat f}\right]^2\right] = E\left[{\hat f}\right]^2$, and
$E\left[{\hat f} E\left[{\hat f}\right]\right] = E\left[{\hat f}\right]^2$, so those also cancel.

So, if we identify $E\left[ \left(f - E\left[{\hat f}\right]\right)^2\right]$ as 
${\rm Bias}^2\left[{\hat f}\right]$ and the second is
$E\left[\left(E\left[{\hat f}\right] - {\hat f}\right)^2\right] = E\left[{\hat f}^2\right] - E\left[{\hat f}\right]^2$,
which is ${\rm Var}\left[{\hat f}\right]$.  So, finally,
$$
E\left[ \left(Y - {\hat f}\right)^2 \right] = {\rm Var}\left[\epsilon\right]
 + {\rm Bias}^2\left[{\hat f}\right] + {\rm Var}\left[{\hat f}\right],
$$
which is the bias-variance decomposition.

\end