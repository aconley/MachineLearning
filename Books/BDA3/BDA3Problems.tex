\font\big=cmbx10 scaled\magstep1
\font\bigger=cmbx10 scaled\magstep3

\topglue 0.5in
\centerline{\bigger Problems from BDA 3}
\vskip 0.5in

\noindent
{\bigger Chapter 1}
\vskip 0.2in

\noindent {\big Problem 1} \hfil \break
\noindent a) $P\left(y\right) \sim 1/2 \left(N\left(\mu_1, \sigma^2\right) + 
N \left(\mu_2, \sigma^2\right)\right)$ \hfil \break
\noindent b) 
$$
\eqalign{
 P \left( \theta = 1 | y \right) &= {P\left(y | \theta = 1\right) P\left(\theta = 1\right)
  \over P\left(y | \theta = 1\right) P\left(\theta = 1\right) +
  P\left(y | \theta = 2\right) P\left(\theta = 2\right)} \cr &=
  \left(1 + \exp \left[ \left( 2 y \left(\mu_2 - \mu_1\right) + 
   \mu_2^2 - \mu_1^2\right) / 2 \sigma^2 \right] \right)^{-1} }.
$$

\noindent {\big Problem 3}\hfil \break
\noindent We have $P\left(xx\right) = p^2$, $P\left(xX\right) = 2 p \left(1-p\right)$
(where we ignore ordering, so that Xx is counted as the same as xX),
and so $P\left(XX\right) = 1 - p^2 - 2 p \left(1-p\right) = \left(1 - p\right)^2$.
Now 
$$
P\left(xX | \rm{brown}\right) =  {1 \cdot P\left(xX\right) \over
1 \cdot P\left(xX\right) + 1 \cdot P\left(XX\right) + 0 \cdot P\left(xx\right)} = 
 {2 p \over 1 + p},
$$
and therefore $P\left(XX | \rm{brown}\right) = \left(1-p\right)/\left(1 + p\right)$.

The complication here is normalization.  So what we want to compute is
$$P\left(xX | p_1=\rm{brown}, p_2 = \rm{brown}\right) \over
P\left(xX | p_1=\rm{brown}, p_2 = \rm{brown}\right) +
P\left(XX | p_1=\rm{brown}, p_2 = \rm{brown}\right).$$
Starting with the first term
$$
\eqalign{
P\left(xX | p_1=\rm{brown}, p_2 = \rm{brown}\right)  &= P\left(xX | p_1=xX, p_2=xX\right)
 P\left(xX\right)^2 \cr &+ 2 P\left(xX | p_1=xX, p_2=XX\right) P\left(xX\right) P\left(XX\right)
 \cr &+ P\left(xX | p_1=xx, p_2=xx\right) P\left(xx\right)^2 \cr 
  &= {1 \over 2} \left({2 p \over 1+p}\right)^2 + 2 {1 \over 2} {2 p \over 1 + p} {1 - p \over 1 + p} + 0
  \left({1 - p \over 1 + p}\right)^2 \cr &= {2 p \over \left(1 + p\right)^2},
}
$$
where things like $P\left(xX | p_1=xX, p_2 = XX\right) = 1/2$ come from inheritance
squares.

Similarly
$$
\eqalign{
P\left(XX | p_1=\rm{brown}, p_2 = \rm{brown}\right)  &= P\left(XX | p_1=xX, p_2=xX\right)
 P\left(xX\right)^2 \cr &+ 2 P\left(XX | p_1=xX, p_2=XX\right) P\left(xX\right) P\left(XX\right)
 \cr &+ P\left(XX | p_1=xx, p_2=xx\right) P\left(xx\right)^2 \cr 
  &= {1 \over 4} \left({2 p \over 1+p}\right)^2 + 2 {1 \over 2} {2 p \over 1 + p} {1 - p \over 1 + p} + 
  1 \cdot \left({1 - p \over 1 + p}\right)^2 \cr &= {1 \over \left(1 + p\right)^2}.
}
$$
Therefore, the steady state fraction is
$${P\left(xX | p_1=\rm{brown}, p_2 = \rm{brown}\right) \over
P\left(xX | p_1=\rm{brown}, p_2 = \rm{brown}\right) +
P\left(XX | p_1=\rm{brown}, p_2 = \rm{brown}\right)} = {2 p \over 1 + 2 p}
$$
as claimed.

If Judy has brown eyed kids with a xX, the outcome depends on Judy's genome:
$$
\eqalign{
 P\left(\rm{child} = \rm{brown} | \rm{Judy}=xX\right) &= P\left(xX | J=xX\right)
 + P\left(XX | J=xX\right) = 1/2 + 1/4 = 3/4 \cr
 P\left(\rm{child} = \rm{brown} | \rm{Judy} = XX\right) &= 1}.
 $$
So, if Judy has $n$ brown eyed children, we have
$$
 P\left(J=xX | n\right) = {P\left(n | xX\right) P\left(xX\right) \over
  P\left(n | xX\right) P\left(xX\right) + P\left(n | XX\right) P\left(XX\right)} =
  {\left(3 \over 4\right)^n 2 p \over \left(3 / 4\right)^n 2 p + 1}
$$

\noindent {\big Problem 6}\hfil \break
\noindent 
The proportion of births that are fraternal twins is 1/125, and 1/4 of those are
both males.  The proportion of identical twins is 1/300, and 1/2 of those are both
males.  Thus
$$
  P\left(i | \rm{both\,male}\right) = 
  {P\left( \rm{both\,male} | i\right) P\left(i\right) \over
   P\left( \rm{both\,male} | i\right) P\left(i\right) +
   P\left( \rm{both\,male} | f\right) P\left(f\right)} = 
    {1/2 \cdot 1/300 \over 1/2 \cdot 1/300 + 1 / 4 \cdot 1 / 125} = {5 \over 11}.
$$

\vskip 0.5in
\noindent
{\bf \bigger Chapter 2}
\vskip 0.2in

\noindent {\big Problem 1} \hfil \break
The prior distribution is $\rm{Beta}\left(4 ,4\right) \propto \theta^3 \left(1 - \theta\right)^3$,
while the probability of fewer than 3 heads conditional on $\theta$ is 
$P\left(n < 3 | \theta\right) = P\left(0 | \theta\right) + P\left(1 | \theta\right) +
P\left(2 | \theta\right) = {10 \choose 0} \left(1 - \theta\right)^{10} +
{10 \choose 1} \theta \left(1 - \theta\right)^9 + {10 \choose 2} \theta^2 \left(1 - \theta\right)^8$.
We therefore have $P\left(\theta | n < 3\right) \propto
{10 \choose 0} \theta^3 \left(1 - \theta\right)^{13} +
{10 \choose 1} \theta^4 \left(1 - \theta\right)^{12} + {10 \choose 2} \theta^5 \left(1 - \theta\right)^{11}$.
Normalizing by requiring $\int_0^1\,d\theta P\left(\theta | n < 3\right) = 1$ gives
$$
 P\left(\theta | n < 3 \right) = {7735 \over 8} \theta^3 \left(1 - \theta\right)^11
  \left(1 + 8 \theta + 36 \theta^2\right).
$$
The mode is 0.2812, the mean $39/128$, and the variance 0.0139.  The 95\% limits
are 0.109 to 0.540.

\vskip 0.2in
\noindent {\big Problem 2} \hfil \break
We have $P\left(H | C_1\right) = 0.6$ and $P\left(H | C_2\right) = 0.4$, so
$$
 P\left(C_1 | T, T\right) = {P\left(T, T | C_1\right) P\left(C_1\right) \over 
  P\left(T, T | C_1\right) P\left(C_1\right) + P\left(T, T | C_2\right) P\left(C_2\right)}.
$$
Now $P\left(T,T | C_1\right) = 0.4^2$ and $P\left(T, T | C_2\right) = 0.6^2$,
so assuming priors $P\left(C_1\right) = P\left(C_2\right) = 1/2$, we have
$P\left(C_1 | T, T\right) = 0.3077$.

The probability of making N spins until a head comes up is the probability of
getting N-1 consecutive tails followed by one head, or $\theta \left(1 - \theta\right)^{N-1}$.
Thus the expected number is therefore (with the aid of Mathematica)
$$
 E\left(N | \theta\right) = {\sum_{N=1}^\infty N \theta \left(1-\theta\right)^{N-1} \over
  \sum_{N=1}^\infty \theta \left(1 - \theta\right)^{N-1}} = {1 \over \theta}.
$$
Therefore, the expectation is $0.3077 / 0.6 + \left(1-0.3077\right) / 0.4 = 2.2436$

Alternatively, we could compute the probability distribution directly and do the sums on
that using
$$
 P\left( \left(N-1\right) T, H | T, T \right) = 0.6 \left(1-0.6\right)^{N-1} \cdot 0.3077
  0.4 \left(1 - 0.4\right)^{N-1} \cdot \left(1 - 0.3077\right),
$$
which gives the same answer.

\vskip 0.2in
\noindent {\big Problem 3} \hfil \break
a) This is just a normal distribution with $\mu = 1000 / 6$ and $\sigma^2 = 5000 / 36$.\hfil\break
b) It has approximate quantiles 147.3, 158.7, 166.7, 174.6, and 186.05 (5, 25, 50, 75, 95).
  The exact (but discrete) quantiles are 147, 159, 167, 175, 186.
  
\vskip 0.2in
\noindent {\big Problem 5} \hfil \break
a) ${\rm Pr}\left(y = k | \theta\right) = {\rm Bin}\left(y | n, \theta\right)$.  This
can be integrated:
$$
  {\rm Pr}\left(y = k\right) = \int_0^1 \, d\theta \, \theta^k \left(1 - \theta\right)^(n - k)
    {n \choose k} = {1 \over n + 1}.
$$
b) The posterior density is 
$$
 p\left(\theta | y\right) = {\rm Beta}\left(\theta | \alpha + y, \beta + n - y\right)
$$
which, from the properties of a Beta distribution, has a mean
$$
  \alpha + y \over \beta + \alpha + n .
$$
The algebra is tedious, so I won't pursue this further.

\vskip 0.2in
\noindent {\big Problem 8} \hfil \break
a) Directly from (2.11), $p\left(\theta | y\right) = N\left(\theta | \mu_n, \tau^2_n\right)$
with
$$
 \mu_n = {{180 \over 40^2} + n {150 \over 20^2} \over {1 \over 40^2} + {n \over 20^2}}
$$
and
$$
 {1 \over \tau_n^2} = {1 \over 40^2} + {n \over 20^2}.
$$
b) This is simple for Normally distributed values -- the mean value is just $\mu_n$
and the variance is $\tau^2_n + 20^2$ (recall, the variance is assumed known).\hfil\break
c) $\mu_n = 150.7$ for $n=10$, and $\tau^2_n = 39.02$.  The 95\% interval
  for $\theta$ is just $\mu_n \pm 1.96 \tau_n$, or about $\left[138.5, 162.9\right]$.
 For $\tilde{y}$ the only thing that changes is the variance becomes $439.02$,
  giving an interval of $\left[109.6, 191.8\right]$.\hfil\break
d) I'll just give the updated means and variances: $\mu_n = 150.1$,
 $\tau^2_n = 3.99$.
 
\vskip 0.2in
\noindent {\big Problem 9} \hfil\break
a) The two equations to solve are ${\rm E} \left(\theta\right) = \alpha / \alpha + \beta = 0.6$
and ${\rm Var} \left(\theta\right) = \alpha \beta / \left(\alpha + \beta\right)^2 \left(\alpha + \beta + 1\right)
= 0.3^2$, which yields $\alpha = 1$, $\beta = 2/3$.\hfil\break
b) $p\left( \theta | y \right) = {\rm Beta}\left(\theta | 651, 350\,2/3\right)$, which
  is sharply peaked near 0.6499 with a variance of $2.25 \times 10^{-4}$.

\vskip 0.2in
\noindent {\big Problem 10} \hfil\break
a) The probability of seeing car 203 is 0 if $N < 203$ and $1/N$ otherwise.  Therefore,
the posterior probability is
$$
 p\left(N | 203\right) \propto \cases{0, & if $N < 203$;\cr
   {1 \over 100} \left({99 \over 100}\right)^{N-1} {1 \over N} & otherwise\cr}.
$$
b) Numerically evaluating $p$ as well as it's normalization, 
${\rm E}\left(N\right) = 279.1$ and ${\rm var}\left(N\right) = 6388$.\hfil\break
c) The standard $1/N$ prior causes the sums to not converge.

\vskip 0.2in
\noindent {\big Problem 13}\hfil\break
a) Using a 'vague' improper prior $p\left(\theta\right) \propto 1 / \theta$ (for $\theta > 0$),
and $p\left(y | \theta\right) = \prod_{1976}^{1985} {1 \over n_f !} \theta^{n_f} e^{-\theta}
= e^{-10\theta} \theta^{237}$.  Multiplying them gives a ${\rm Gamma}\left(238, 10\right)$
distribution for $p\left(\theta | y\right)$.  So to generate a new draw by generating
$\theta$ from that distribution, and then the predicted number is ${\rm Poisson}\left(\theta\right)$.
A quick numerical calculation gives 14-34.

\vskip 0.2in
\noindent {\big Problem 14} \hfil \break
a) It suffices to consider $\left(y - \theta\right)^2/\sigma^2 + \left(\theta - \mu_0\right)^2/\tau_0^2$.
Expanding this, and discarding any terms that do not contain $\theta$, since they can be
absorbed into the proportioinality, gives $\theta^2 \left(1/\sigma^2 + 1/\tau_0^2\right) -
2 \theta \left(y / \sigma^2 + \mu_0 / \tau_0^2\right)$.  Now, complete the square
using the standard $a x^2 + b x = a\left(x + d\right)^2 + e$ where $d = b/2a$ and
$e = -b^2 / 4a$ with $a = \left(1 / \sigma^2 + 1 / \tau_0^2\right)$ and 
$b = -2 \left(y / \sigma^2 + \mu_0 / \tau_0^2\right)$.  This $e$ doesn't depend on $\theta$,
and can be discarded, leaving
$$
 p\left(\theta | y\right) \propto \exp \left(-{1 \over 2} \left({1 \over \sigma^2} + {1 \over \tau_0^2}
 \right) \left(\theta - {y / \sigma^2 + \mu_0 / \tau_0^2 \over 1 / \sigma^2 + 1 / \tau_0^2}
 \right)^2\right),
$$
which is formulae 2.9 and 2.10.\hfil\break
b) To derive 2.12, just use the formula
$$
 \sum \left(y_i - \mu\right)^2 = \sum \left(y_i - \bar{y}\right)^2 + n \sum \left(\bar{y} - \mu\right)^2
$$
and, since the final term doesn't depend on $\mu$, this is the same as before but
with $y \mapsto \bar{y}$ and $\sigma^2 \mapsto \sigma^2 / n$.

\vskip 0.5in
\noindent
{\bf \bigger Chapter 3}
\vskip 0.2in

\noindent {\big Problem 1} \hfil \break
Denote the prior by ${\rm Dirchlet}\left(\beta_1, \ldots, \beta_J\right)$.
Then the posterior pdf will be 
$$
 p\left(\theta_1, \ldots, \theta_J | y\right) =
{\rm Dirchlet}\left(\beta_1 + y_1, \ldots, \beta_J + y_J\right)
 \propto \theta_1^{\beta_1 + y_1 - 1} \ldots \theta_J^{\beta_J + y_J - 1}.
$$
We will need to find $p\left(\theta_1, \theta_2 | y\right)$ to do the transform.
Examine what happens when one integral is done:
$$
\eqalign{
  p\left(\theta_1, \ldots \theta_{J-1} | y\right) 
    &= \int\, d\theta_J\,\, p\left(\theta_1, \ldots, \theta_J | y\right) \cr
    &\propto \int \, d\theta_J\,\, \theta_1^{\beta_1 + y_1 - 1} \ldots 
         \theta_J^{\beta_J + y_J - 1} \cr
    &= \int_0^{1 - \theta_1 - \ldots - \theta_{J-1}}\, d\theta_J\,\,
          \theta_1^{\beta_1 + y_1 - 1} \ldots \theta_J^{\beta_J + y_J - 1} \cr
    &= {1 \over \beta_J + y_J} 
        \left(1 - \theta_1 - \ldots \theta_{J-1}\right)^{\beta_J + y_J}
        \theta_1^{\beta_1 + y_1 - 1} \ldots \theta_{J-1}^{\beta_{J-1} + y_{J-1} - 1}
        . \cr}
$$
Do this $J-2$ times to get the marginal distribution.  Actually, even better,
note that in Appendix~A, it flat out states that the marginal distribution
of the subvector is 
$$
 p\left(\theta_1, \theta_2 | y\right) = {\rm Dirchlet}
\left(\beta_1 + y_1, \beta_2 + y_2, \beta_3 + \ldots + \beta_J + y_3 + \ldots + y_J
\right)
$$ 
where the last variable is $1 - \theta_1 - \theta_2$; that is:
$$
 p\left(\theta_1, \theta_2 | y\right) \propto \theta_1^{\beta_1 + y_1 - 1}
   \theta_2^{\beta_2 + y_2 - 1} \left(1 - \theta_1 - \theta2\right)^{
      \beta_3 + \ldots + \beta_J + y_3 + \ldots + y_J - 1}.
$$
Now do a change of variable.  We need to chose another one to pair with $\alpha =
\theta_1 / \theta_1 + \theta_2$; $\gamma = \theta_1 + \theta_2$ is a reasonable choice
giving $\theta_1 = \alpha \gamma$ and $\theta_2 = \gamma \left(1 - \alpha\right)$.
The Jacobian is
$$
 J = \left| \matrix{{\partial \theta_1 \over \partial \alpha} &
                    {\partial \theta_2 \over \partial \alpha} \cr
                    {\partial \theta_1 \over \partial \gamma} &
                    {\partial \theta_2 \over \partial \gamma}}\right| =
     \left| \matrix{\gamma & -\gamma \cr \alpha & 1 - \alpha} \right| =
      \gamma .
$$
Thus
$$
\eqalign{
 p\left(\alpha, \gamma | y\right) &\propto \gamma 
   \left(\alpha \gamma\right)^{\beta_1 + y_1- 1}
   \left(\gamma \left(1 - \alpha\right)\right)^{\beta_2 + y_2 - 1}
   \left(1 - \gamma\right)^{\beta_3 + \ldots + \beta_J + y_3 + \ldots + y_J - 1} \cr
   &= \alpha^{\beta_1 + y_1 - 1} \left(1 - \alpha\right)^{\beta_2 + y_2 -1}
     g\left(\gamma\right),\cr
   }     
$$
where $g$ is some function of $\gamma$ but not $\alpha$.
Since it factors into two independent pieces, we then have
$p \left(\alpha | y\right) \propto \alpha^{\beta_1 + y_1 - 1}$
which means that we must have $p \left(\alpha | y\right) = {\rm Beta}
\left(\alpha | \beta_1 + y_1, \beta_2 + y_2\right)$.
\end